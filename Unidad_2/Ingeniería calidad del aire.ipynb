{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext,HiveContext\n",
    "from pyspark.sql import SQLContext,SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta = '/aire'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hdfs:///aire/2010CO.csv',\n",
       " 'hdfs:///aire/2010NO.csv',\n",
       " 'hdfs:///aire/2010NO2.csv',\n",
       " 'hdfs:///aire/2010NOX.csv',\n",
       " 'hdfs:///aire/2010O3.csv',\n",
       " 'hdfs:///aire/2010PM10.csv',\n",
       " 'hdfs:///aire/2010PM25.csv',\n",
       " 'hdfs:///aire/2010SO2.csv',\n",
       " 'hdfs:///aire/2011CO.csv',\n",
       " 'hdfs:///aire/2011NO.csv',\n",
       " 'hdfs:///aire/2011NO2.csv',\n",
       " 'hdfs:///aire/2011NOX.csv',\n",
       " 'hdfs:///aire/2011O3.csv',\n",
       " 'hdfs:///aire/2011PM10.csv',\n",
       " 'hdfs:///aire/2011PM25.csv',\n",
       " 'hdfs:///aire/2011PMCO.csv',\n",
       " 'hdfs:///aire/2011SO2.csv',\n",
       " 'hdfs:///aire/2012CO.csv',\n",
       " 'hdfs:///aire/2012NO.csv',\n",
       " 'hdfs:///aire/2012NO2.csv',\n",
       " 'hdfs:///aire/2012NOX.csv',\n",
       " 'hdfs:///aire/2012O3.csv',\n",
       " 'hdfs:///aire/2012PM10.csv',\n",
       " 'hdfs:///aire/2012PM25.csv',\n",
       " 'hdfs:///aire/2012PMCO.csv',\n",
       " 'hdfs:///aire/2012SO2.csv',\n",
       " 'hdfs:///aire/2013CO.csv',\n",
       " 'hdfs:///aire/2013NO.csv',\n",
       " 'hdfs:///aire/2013NO2.csv',\n",
       " 'hdfs:///aire/2013NOX.csv',\n",
       " 'hdfs:///aire/2013O3.csv',\n",
       " 'hdfs:///aire/2013PM10.csv',\n",
       " 'hdfs:///aire/2013PM25.csv',\n",
       " 'hdfs:///aire/2013PMCO.csv',\n",
       " 'hdfs:///aire/2013SO2.csv',\n",
       " 'hdfs:///aire/2014CO.csv',\n",
       " 'hdfs:///aire/2014NO.csv',\n",
       " 'hdfs:///aire/2014NO2.csv',\n",
       " 'hdfs:///aire/2014NOX.csv',\n",
       " 'hdfs:///aire/2014O3.csv',\n",
       " 'hdfs:///aire/2014PM10.csv',\n",
       " 'hdfs:///aire/2014PM25.csv',\n",
       " 'hdfs:///aire/2014PMCO.csv',\n",
       " 'hdfs:///aire/2014SO2.csv',\n",
       " 'hdfs:///aire/2015CO.csv',\n",
       " 'hdfs:///aire/2015NO.csv',\n",
       " 'hdfs:///aire/2015NO2.csv',\n",
       " 'hdfs:///aire/2015NOX.csv',\n",
       " 'hdfs:///aire/2015O3.csv',\n",
       " 'hdfs:///aire/2015PM10.csv',\n",
       " 'hdfs:///aire/2015PM25.csv',\n",
       " 'hdfs:///aire/2015PMCO.csv',\n",
       " 'hdfs:///aire/2015SO2.csv',\n",
       " 'hdfs:///aire/2016CO.csv',\n",
       " 'hdfs:///aire/2016NO.csv',\n",
       " 'hdfs:///aire/2016NO2.csv',\n",
       " 'hdfs:///aire/2016NOX.csv',\n",
       " 'hdfs:///aire/2016O3.csv',\n",
       " 'hdfs:///aire/2016PM10.csv',\n",
       " 'hdfs:///aire/2016PM25.csv',\n",
       " 'hdfs:///aire/2016PMCO.csv',\n",
       " 'hdfs:///aire/2016SO2.csv',\n",
       " 'hdfs:///aire/2017CO.csv',\n",
       " 'hdfs:///aire/2017NO.csv',\n",
       " 'hdfs:///aire/2017NO2.csv',\n",
       " 'hdfs:///aire/2017NOX.csv',\n",
       " 'hdfs:///aire/2017O3.csv',\n",
       " 'hdfs:///aire/2017PM10.csv',\n",
       " 'hdfs:///aire/2017PM25.csv',\n",
       " 'hdfs:///aire/2017PMCO.csv',\n",
       " 'hdfs:///aire/2017SO2.csv',\n",
       " 'hdfs:///aire/2018CO.csv',\n",
       " 'hdfs:///aire/2018NO.csv',\n",
       " 'hdfs:///aire/2018NO2.csv',\n",
       " 'hdfs:///aire/2018NOX.csv',\n",
       " 'hdfs:///aire/2018O3.csv',\n",
       " 'hdfs:///aire/2018PM10.csv',\n",
       " 'hdfs:///aire/2018PM25.csv',\n",
       " 'hdfs:///aire/2018PMCO.csv',\n",
       " 'hdfs:///aire/2018SO2.csv',\n",
       " 'hdfs:///aire/2019CO.csv',\n",
       " 'hdfs:///aire/2019NO.csv',\n",
       " 'hdfs:///aire/2019NO2.csv',\n",
       " 'hdfs:///aire/2019NOX.csv',\n",
       " 'hdfs:///aire/2019O3.csv',\n",
       " 'hdfs:///aire/2019PM10.csv',\n",
       " 'hdfs:///aire/2019PM25.csv',\n",
       " 'hdfs:///aire/2019PMCO.csv',\n",
       " 'hdfs:///aire/2019SO2.csv',\n",
       " 'hdfs:///aire/2020CO.csv',\n",
       " 'hdfs:///aire/2020NO.csv',\n",
       " 'hdfs:///aire/2020NO2.csv',\n",
       " 'hdfs:///aire/2020NOX.csv',\n",
       " 'hdfs:///aire/2020O3.csv',\n",
       " 'hdfs:///aire/2020PM10.csv',\n",
       " 'hdfs:///aire/2020PM25.csv',\n",
       " 'hdfs:///aire/2020PMCO.csv',\n",
       " 'hdfs:///aire/2020SO2.csv',\n",
       " 'hdfs:///aire/2021CO.csv',\n",
       " 'hdfs:///aire/2021NO.csv',\n",
       " 'hdfs:///aire/2021NO2.csv',\n",
       " 'hdfs:///aire/2021NOX.csv',\n",
       " 'hdfs:///aire/2021O3.csv',\n",
       " 'hdfs:///aire/2021PM10.csv',\n",
       " 'hdfs:///aire/2021PM25.csv',\n",
       " 'hdfs:///aire/2021PMCO.csv',\n",
       " 'hdfs:///aire/2021SO2.csv']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = 'hdfs dfs -ls %s'%ruta\n",
    "lst = subprocess.check_output(cmd, shell=True).decode('utf8').strip().split('\\n')\n",
    "lst = [x.split(' ')[-1] for x in lst]\n",
    "lst = sorted(['hdfs://%s'%x for x in lst if (x!='items')&(x[-3:]=='csv')])\n",
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession = SparkSession.builder.appName(\"bigdatita\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sparkSession.read.csv(lst,inferSchema=False,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FECHA: string (nullable = true)\n",
      " |-- HORA: string (nullable = true)\n",
      " |-- ESTACION: string (nullable = true)\n",
      " |-- VALOR: string (nullable = true)\n",
      " |-- CONTAMINANTE: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a fecha y hora\n",
    "df = df.withColumn('FECHA',F.to_timestamp(F.col('FECHA')))# Se usan las funciones que están en F\n",
    "df = df.withColumn('HORA',F.col('HORA').cast('int'))\n",
    "df = df.withColumn('VALOR',F.col('VALOR').cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FECHA: timestamp (nullable = true)\n",
      " |-- HORA: integer (nullable = true)\n",
      " |-- ESTACION: string (nullable = true)\n",
      " |-- VALOR: double (nullable = true)\n",
      " |-- CONTAMINANTE: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos catalogos de fecha y hora para poder poner un sencillo identificador en el df principal\n",
    "# Se convierte a pandas por que es más fácil\n",
    "catfh = df.select('FECHA','HORA').drop_duplicates().toPandas()\n",
    "catfh = catfh.sort_values(by=['FECHA','HORA']).reset_index(drop=True)\n",
    "catfh['id'] = catfh.index+1\n",
    "anclai,anclaf = catfh['id'].min(), catfh['id'].max()\n",
    "catfh = spark.createDataFrame(catfh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(catfh,['FECHA','HORA'],'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('FECHA','HORA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ESTACION: string (nullable = true)\n",
      " |-- VALOR: double (nullable = true)\n",
      " |-- CONTAMINANTE: string (nullable = true)\n",
      " |-- id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 97847)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vobs = 1000\n",
    "vdes =  1\n",
    "anclai,anclaf = anclai+vobs-1,anclaf-vdes\n",
    "anclai,anclaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+------------+---+\n",
      "|ESTACION|VALOR|CONTAMINANTE| id|\n",
      "+--------+-----+------------+---+\n",
      "|     VAL|  0.0|         SO2| 76|\n",
      "|     SUR|  0.0|         SO2| 76|\n",
      "|     TAC|  2.0|         SO2| 76|\n",
      "|     FAC|  2.0|         SO2| 76|\n",
      "|     LLA| null|         SO2| 76|\n",
      "+--------+-----+------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ahora la idea es predecir el valor que tendra un dia después cada contaminante por estacion/id(datetime)\n",
    "a partir de mil oobservaciones,\n",
    "para ello creamos varias columnas con agregados\n",
    "'''\n",
    "def ing(df,k,ancla):\n",
    "    aux = df.filter((df['id']>=(ancla-k+1))&(df['id']<=ancla))\n",
    "    expr = [y(F.col('VALOR')).alias(f'x_{z}_{k}') for y,z in zip([F.min,F.max,F.mean,F.stddev],\n",
    "                                                             ['minimo','maximo','media','desv'])]\n",
    "    aux = aux.groupBy('ESTACION').pivot('CONTAMINANTE').agg(*expr).withColumn('ancla',F.lit(ancla))\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "um = ['ESTACION','ancla']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "step = 100\n",
    "aux = reduce(lambda x,y:x.join(y,um,'outer'),map(lambda k:ing(df,k,1000),range(step,vobs+step,step)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reduce(lambda x,y:x.union(y),map(lambda ancla:reduce(lambda x,y:x.join(y,um,'outer'),\n",
    "                    map(lambda k:ing(df,k,ancla),range(step,vobs+step,step))),range(anclai,anclaf+1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
