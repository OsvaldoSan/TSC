{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd \n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql import functions as F\n","from pyspark.ml.feature import VectorAssembler\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","\n","from functools import reduce"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["df = spark.read.csv(\"hdfs://bigdatita-m:8020/chicago\",header=True,inferSchema=True)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["root\n"," |-- taxi_id: string (nullable = true)\n"," |-- trip_start_timestamp: string (nullable = true)\n"," |-- trip_end_timestamp: string (nullable = true)\n"," |-- trip_seconds: string (nullable = true)\n"," |-- trip_miles: string (nullable = true)\n"," |-- pickup_census_tract: string (nullable = true)\n"," |-- dropoff_census_tract: string (nullable = true)\n"," |-- pickup_community_area: string (nullable = true)\n"," |-- dropoff_community_area: string (nullable = true)\n"," |-- fare: string (nullable = true)\n"," |-- tips: string (nullable = true)\n"," |-- tolls: string (nullable = true)\n"," |-- extras: string (nullable = true)\n"," |-- trip_total: string (nullable = true)\n"," |-- payment_type: string (nullable = true)\n"," |-- company: string (nullable = true)\n"," |-- pickup_latitude: string (nullable = true)\n"," |-- pickup_longitude: string (nullable = true)\n"," |-- dropoff_latitude: string (nullable = true)\n"," |-- dropoff_longitude: string (nullable = true)\n","\n"]}],"source":["df.printSchema()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["df = df.withColumn(\"fh\",F.to_timestamp(F.col(\"trip_start_timestamp\")))\n","df = df.withColumn(\"week\",F.weekofyear(F.col(\"fh\")))"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["df = df.select(*[\"taxi_id\",\"week\",\"trip_total\",\"tips\"]).withColumn(\"n\",F.lit(1))"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["(1, 53)"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["cat = df.select(\"week\").drop_duplicates().toPandas()\n","cat = cat.sort_values(by=\"week\").reset_index(drop=True)\n","wini,wfin = cat[\"week\"].min(), cat[\"week\"].max()\n","cat = spark.createDataFrame(cat)\n","wini,wfin"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":["(8, 50)"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["vobs = 8\n","vdes = 3\n","anclai,anclaf = wini+vobs-1, wfin-vdes\n","anclai,anclaf "]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["df = df.withColumn(\"ratio_tips\",F.col(\"tips\")/F.col(\"trip_total\"))"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["varc = [\"trip_total\",\"tips\",\"n\",\"ratio_tips\"]"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def ing_X(df,varc,k,ancla):\n","    aux = df.filter((df[\"week\"]<=ancla)&(df[\"week\"]>=(ancla-k+1)))\n","    expr = [y(x).alias(f\"v_{z}_{x}_{k}\") for x in varc for y,z in zip([F.sum,F.min,F.max,F.avg,F.stddev],\n","                                                                      [\"suma\",\"minimo\",\"maximo\",\"media\",\"desv\"]\n","                                                                     )]\n","    aux = aux.groupBy(\"taxi_id\").agg(*expr).withColumn(\"ancla\",F.lit(ancla))\n","    return aux"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["step = 2"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 2.19 s, sys: 833 ms, total: 3.02 s\n","Wall time: 15.7 s\n"]}],"source":["%%time\n","X = reduce(lambda x,y:x.union(y),map(lambda ancla:reduce(lambda x,y:x.join(y,[\"taxi_id\",\"ancla\"],\"outer\"),\n","       map(lambda k:ing_X(df,varc,k,ancla),range(step,vobs+step,step))),range(anclai,anclaf+1)))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["def ing_y(df,vdes,ancla):\n","    aux = df.filter((df[\"week\"]>ancla)&(df[\"week\"]<=(ancla+vdes))).select(\"taxi_id\").drop_duplicates()\n","    aux = aux.withColumn(\"target\",F.lit(0)).withColumn(\"ancla\",F.lit(ancla))\n","    return aux"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["y = reduce(lambda x,y:x.union(y),map(lambda ancla:ing_y(df,vdes,ancla),range(anclai,anclaf+1)))"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["tad = X.join(y,[\"taxi_id\",\"ancla\"],\"left\")"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["tad  = tad.fillna({\"target\":1})"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["var = [v for v in tad.columns if v[:2]==\"v_\"]"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["assembler = VectorAssembler(inputCols=var,outputCol=\"features\")"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"ename":"IllegalArgumentException","evalue":"'Data type string of column v_minimo_trip_total_2 is not supported.\\nData type string of column v_maximo_trip_total_2 is not supported.\\nData type string of column v_minimo_tips_2 is not supported.\\nData type string of column v_maximo_tips_2 is not supported.\\nData type string of column v_minimo_trip_total_4 is not supported.\\nData type string of column v_maximo_trip_total_4 is not supported.\\nData type string of column v_minimo_tips_4 is not supported.\\nData type string of column v_maximo_tips_4 is not supported.\\nData type string of column v_minimo_trip_total_6 is not supported.\\nData type string of column v_maximo_trip_total_6 is not supported.\\nData type string of column v_minimo_tips_6 is not supported.\\nData type string of column v_maximo_tips_6 is not supported.\\nData type string of column v_minimo_trip_total_8 is not supported.\\nData type string of column v_maximo_trip_total_8 is not supported.\\nData type string of column v_minimo_tips_8 is not supported.\\nData type string of column v_maximo_tips_8 is not supported.'","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)","\u001B[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m     62\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 63\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkw\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     64\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mpy4j\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprotocol\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPy4JJavaError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001B[0m in \u001B[0;36mget_return_value\u001B[0;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[1;32m    327\u001B[0m                     \u001B[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 328\u001B[0;31m                     format(target_id, \".\", name), value)\n\u001B[0m\u001B[1;32m    329\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;31mPy4JJavaError\u001B[0m: An error occurred while calling o10695.transform.\n: java.lang.IllegalArgumentException: Data type string of column v_minimo_trip_total_2 is not supported.\nData type string of column v_maximo_trip_total_2 is not supported.\nData type string of column v_minimo_tips_2 is not supported.\nData type string of column v_maximo_tips_2 is not supported.\nData type string of column v_minimo_trip_total_4 is not supported.\nData type string of column v_maximo_trip_total_4 is not supported.\nData type string of column v_minimo_tips_4 is not supported.\nData type string of column v_maximo_tips_4 is not supported.\nData type string of column v_minimo_trip_total_6 is not supported.\nData type string of column v_maximo_trip_total_6 is not supported.\nData type string of column v_minimo_tips_6 is not supported.\nData type string of column v_maximo_tips_6 is not supported.\nData type string of column v_minimo_trip_total_8 is not supported.\nData type string of column v_maximo_trip_total_8 is not supported.\nData type string of column v_minimo_tips_8 is not supported.\nData type string of column v_maximo_tips_8 is not supported.\n\tat org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:169)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:86)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n","\nDuring handling of the above exception, another exception occurred:\n","\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)","\u001B[0;32m<ipython-input-19-0777a388f2ab>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0massembler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtad\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m","\u001B[0;32m/usr/lib/spark/python/pyspark/ml/base.py\u001B[0m in \u001B[0;36mtransform\u001B[0;34m(self, dataset, params)\u001B[0m\n\u001B[1;32m    171\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcopy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    172\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 173\u001B[0;31m                 \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    174\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    175\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Params must be a param map but got %s.\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/ml/wrapper.py\u001B[0m in \u001B[0;36m_transform\u001B[0;34m(self, dataset)\u001B[0m\n\u001B[1;32m    310\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_transform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    311\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_transfer_params_to_java\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 312\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_java_obj\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_jdf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msql_ctx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    313\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    314\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1255\u001B[0m         \u001B[0manswer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgateway_client\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend_command\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcommand\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1256\u001B[0m         return_value = get_return_value(\n\u001B[0;32m-> 1257\u001B[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001B[0m\u001B[1;32m   1258\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1259\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mtemp_arg\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtemp_args\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;32m/usr/lib/spark/python/pyspark/sql/utils.py\u001B[0m in \u001B[0;36mdeco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m     77\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mQueryExecutionException\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m': '\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstackTrace\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     78\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstartswith\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'java.lang.IllegalArgumentException: '\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 79\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mIllegalArgumentException\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m': '\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstackTrace\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     80\u001B[0m             \u001B[0;32mraise\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     81\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mdeco\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n","\u001B[0;31mIllegalArgumentException\u001B[0m: 'Data type string of column v_minimo_trip_total_2 is not supported.\\nData type string of column v_maximo_trip_total_2 is not supported.\\nData type string of column v_minimo_tips_2 is not supported.\\nData type string of column v_maximo_tips_2 is not supported.\\nData type string of column v_minimo_trip_total_4 is not supported.\\nData type string of column v_maximo_trip_total_4 is not supported.\\nData type string of column v_minimo_tips_4 is not supported.\\nData type string of column v_maximo_tips_4 is not supported.\\nData type string of column v_minimo_trip_total_6 is not supported.\\nData type string of column v_maximo_trip_total_6 is not supported.\\nData type string of column v_minimo_tips_6 is not supported.\\nData type string of column v_maximo_tips_6 is not supported.\\nData type string of column v_minimo_trip_total_8 is not supported.\\nData type string of column v_maximo_trip_total_8 is not supported.\\nData type string of column v_minimo_tips_8 is not supported.\\nData type string of column v_maximo_tips_8 is not supported.'"]}],"source":["v = assembler.transform(tad)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["mod = LogisticRegression(featuresCol=\"features\",labelCol=\"target\")"]},{"cell_type":"markdown","metadata":{},"source":["tad.toPandas().to_csv(\"taxi_churn.csv\",index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Coefficients: \" + str(mod.coefficients))\n","print(\"Intercept: \" + str(mod.intercept))"]}],"metadata":{"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":2}